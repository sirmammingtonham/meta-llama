{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'inputs': 'Determine whether the given sequence of parentheses is properly matched.\\n\\nSequence: } [ ] }\\nValid/Invalid?', 'targets': ['Invalid'], 'multiple_choice_targets': ['Valid', 'Invalid'], 'multiple_choice_scores': [0, 1], 'idx': 158}, {'inputs': 'Determine whether the given sequence of parentheses is properly matched.\\n\\nSequence: [ ) ( ) } { ) } } (\\nValid/Invalid?', 'targets': ['Invalid'], 'multiple_choice_targets': ['Valid', 'Invalid'], 'multiple_choice_scores': [0, 1], 'idx': 551}, {'inputs': 'Determine whether the given sequence of parentheses is properly matched.\\n\\nSequence: { { [ ) } } [ } ( )\\nValid/Invalid?', 'targets': ['Invalid'], 'multiple_choice_targets': ['Valid', 'Invalid'], 'multiple_choice_scores': [0, 1], 'idx': 141}, {'inputs': 'Given a list of integers, find the maximum sum of any contiguous subarray.\\n\\nList: [1, -2, 3, 4, -5, 8]\\nMaximum sum:', 'targets': ['10'], 'multiple_choice_targets': ['-2', '3', '10', '12', '13']}, {'inputs': 'Given a string, determine whether it is a palindrome.\\n\\nString: racecar\\nPalindrome/Not Palindrome?', 'targets': ['Palindrome'], 'multiple_choice_targets': ['Palindrome', 'Not Palindrome']}, {'inputs': 'Given a list of integers, find the second largest number.\\n\\nList: [1, 5, 3, 2, 4]\\nSecond largest number:', 'targets': ['4'], 'multiple_choice_targets': ['1', '2', '3', '4', '5']}, {'inputs': 'Given a list of integers, find the sum of all even numbers.\\n\\nList: [1, 2, 3, 4, 5, 6, 7, 8, 9]\\nSum of even numbers:', 'targets': ['20'], 'multiple_choice_targets': ['10', '12', '14', '16', '18', '20']}, {'inputs': 'Given a list of integers, find the product of all odd numbers.\\n\\nList: [1, 2, 3, 4, 5, 6, 7, 8, 9]\\nProduct of odd numbers:', 'targets': ['945'], 'multiple_choice_targets': ['45', '105', '315', '945', '1260']}, {'inputs': 'Given a list of integers, find the median value.\\n\\nList: [1, 2, 3, 4, 5, 6, 7, 8, 9]\\nMedian value:', 'targets': ['5'], 'multiple_choice_targets': ['3', '4', '5', '6', '7']}, {'inputs': 'Given a list of integers, find the number that appears most frequently.\\n\\nList: [1, 2, 3, 4, 5, 6, 7, 8, 9, 2, 2, 3, 3, 3]\\nMost frequent number:', 'targets': ['3'], 'multiple_choice_targets': ['2', '3', '4', '5', '6']}]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "from rouge_score import rouge_scorer\n",
    "# input --> test_prompt.json \n",
    "# output --> generated_questions.json \n",
    "\n",
    "with open(\"test_prompt.json\", \"r\") as f:\n",
    "  db_init = json.load(f)\n",
    "f.close()\n",
    "with open(\"generated_questions.json\", \"r\") as f:\n",
    "  generated_questions = json.load(f)\n",
    "f.close()\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer([\"rougeL\"], use_stemmer=False) # same as self-instruct setting\n",
    "# for each input compute rouge-l score with each output from generated questions\n",
    "db_inputs = [x['inputs'] for x in db_init]\n",
    "q_inputs = [x['inputs'] for x in generated_questions]\n",
    "\n",
    "\n",
    "scores = defaultdict(list)\n",
    "excluded_qs = set()\n",
    "for i, existing_q in enumerate(db_inputs):\n",
    "  for j, new_q in enumerate(q_inputs):\n",
    "    score = scorer.score(new_q, existing_q)['rougeL'].fmeasure\n",
    "    scores[j].append((i, score))\n",
    "    if score > 0.7:\n",
    "      excluded_qs.add(j)\n",
    "\n",
    "# filter poor rouge score qs\n",
    "added_questions = [q for i, q in enumerate(generated_questions) if i not in excluded_qs]\n",
    "\n",
    "# append to init_db json\n",
    "\n",
    "with open(\"test_prompt.json\", \"r\") as f:\n",
    "  db_init = json.load(f)\n",
    "  db_init.extend(added_questions)\n",
    "f.close()\n",
    "with open(\"test_prompt.json\", \"w\") as f:\n",
    "  print(db_init)\n",
    "  json.dump(db_init, f)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = None \n",
    "train_dir_gen = None\n",
    "def _rougel_check(task:str, questions):\n",
    "  \"\"\"\n",
    "  task: task name\n",
    "  questions: generated question list in dict format (e.g., \n",
    "  {inputs:<str>, targets:<str>, multiple_choice_targets:<str>})\n",
    "  \"\"\"\n",
    "  # get current examples in db\n",
    "  with open(f\"{train_dir}/{task}.json\", \"r\") as f:\n",
    "    db_init = json.load(f)\n",
    "  f.close()\n",
    "  with open(f\"{train_dir_gen}/{task}.json\", \"r\") as f:\n",
    "    existing_generated_questions = json.load(f)\n",
    "  f.close()\n",
    "\n",
    "  db_init.extend(existing_generated_questions)\n",
    "\n",
    "  db_inputs = [question['inputs'] for question in db_init]\n",
    "  generated_inputs = [question['inputs'] for question in questions]\n",
    "\n",
    "  scorer = rouge_scorer.RougeScorer([\"rougeL\"], use_stemmer=False) # same as self-instruct setting\n",
    "  scores, excluded_qs = defaultdict(list), set()\n",
    "  for i, existing_q in enumerate(db_inputs):\n",
    "    for j, new_q in enumerate(generated_inputs):\n",
    "      score = scorer.score(new_q, existing_q)['rougeL'].fmeasure\n",
    "      scores[j].append((i, score))\n",
    "      if score > 0.7:\n",
    "        excluded_qs.add(j)\n",
    "\n",
    "  return [q for i, q in enumerate(questions) if i not in excluded_qs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "filter_summary = {\"parse_filters\" : 1,\"field_check_filters\" : 0, \"rougel_filters\" : 2}\n",
    "task = \"test\"\n",
    "with open(f\"test.csv\", \"a+\") as f:\n",
    "  # task, parse filter, rl filter, fc filter <-- columns\n",
    "  filter_summary[\"task\"] = task\n",
    "  dict_writer = csv.DictWriter(\n",
    "      f,\n",
    "      fieldnames=[\n",
    "          \"task\",\n",
    "          \"parse_filters\",\n",
    "          \"field_check_filters\",\n",
    "          \"rougel_filters\",\n",
    "      ],\n",
    "  )\n",
    "  dict_writer.writeheader()\n",
    "  dict_writer.writerow(filter_summary)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
